* Kafka 知识点

** 基本概念

broker： 消息中间件的处理节点，一个kafka 节点对应一个 broker，多个 broker 组成一个 kafka 集群。
topic： 主题，kafka 根据topic 对消息进行归类，发不到 kafka 集群的每条调戏都需要指定一个 topic
producer： 消息生产者，向broker发送消息的客户端
consumer: 消息的消费者 从broker读取消息的客户端
consumerGroup： 每个consumer属于一个特定的 consumenrGroup 一条消息可以发送到多个不同的
consumerGroup，但是一个 consumerGroup 只能有一个 consumer 能够消费该消息
Partition： 一个 topic 可以分多个 partition 每个 partition 是内部有序的。一个 partition 只能
被consumer group 中的至多一个 consumer 所绑定消费，正常情况下，期望 partition 的数量能够与
consumerGropup 真的数量均等，这样能做到负载均衡。

** 常见特性

对于 producer 来说，可以设置同一个消息不会被多次投递，这依赖于 broker 给每个producer 一个编号，
以及对 message 也有相关的编号，所以会对相同的编号消息进行去重复。
Kafka 同样支持 transaction message， 即 batch message 要么全部写入成功，要么全部写入失败

** kafka 的三种模式

*** 注册方式： 
1. subscribe 方式： 当主题分区数量变化或者 consumer 数量变化时，会进行 rebalance；注册rebalance
监听器，可以手动管理 offset 不注册监听器， kafka 自动管理
2. assign 方式： 手动将 consumer 与 partition 进行对应， kafka 不会进行 rebalance

*** 关键配置：
enable.auto.commit 是否自动提交自己的 offset值；默认时是 true
auto.commit.interval.ms 自动提交时长间隔；默认值是 5000ms
consumer.commitsync(); offset 提交命令；

默认配置

采用默认配置的情况下，既不能完全保证 at-least-once 也不能完全保证 at-most-once

如： 自动提交之后，数据消费流程失败，这样就会有丢失，不能保证 at-least-once；
数据消费成功，但是自动提交失败，可能会导致重复消费，这样也不能呢保证 at-most-once；
但是将自动提交时常设置的足够小，则可以最大限度的保证 at-most-once；

*** At most once 模式

基本思想是保证每一条消息 commit 成功之后，在进行消费处理；
设置自动提交为 false，接收到消息之后，首先 commit，然后再进行消费

*** at least once 模式

基本思想是保证每一条消息处理成功之后，在进行commit；
设置自动提交为 false 消息处理成功之后，手动进行commit；
采用这种模式时，最好保证消费操作时的“幂等性”，防止重复消费。

*** exactly once 模式

核心事项时候将 offset 作为唯一id 与消息同时处理，并保证处理的原子性；设置自动提交为 false；消息处
里成功之后在提及哦啊

** 什么是 kafka 

分布式的消息订阅系统，可以用来处理流式数据的。

** kafka 的好处，可以用卡夫卡来做什么

1. 可以进行消峰处理： 上游数据突发流量，下游可能扛不住，或者下游没有足够的处理能力， kafka 可以在在中间
起到一个缓冲的作用， 下游的服务可以按照自己的节奏慢慢处理。
2. 解耦和扩展： 项目开始的时候不确定具体需求，消息队列可以作为接口，解耦业务流程
3. 冗余：可以采用 一对多的方式，一个生产者发布消息，可以被多个订阅的 topci 的服务消费到，供多个毫无关联
的业务使用
4. 健壮性： 消息队列可以堆积请求，消费端业务即使 宕机，也不影响主要的业务流程
5. 异步通信： 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制。可以讲消息放入队列，然后
在需要的时候进行处理

*** ISR AR 又代表什么？ ISR 伸缩指什么？

:: 需要花时间理解一下

** kafka 中的 zookeeper 起到什么作用，可以不用 zookeeper 么？

zookeeper 是一个分布式协调组件，早期版本 kafka 用 zk 存储 meta 信息存储， consumer 消费状态，group 管理，以及
offset 的值。  zookeeeper 在 kafka 中来选举 controller 和检测 broker 是否存活。

** kafka 的 follower 如何与 leader 同步数据

kafka 的复制机制 不是完全同步，也不是异步复制， 完全同步的话需要，所有follower 复制完成之后才会进行提交
这样性能会有影响，如果是异步复制， 当leader 写入 log 就 commit 这样的话 leader 挂掉，会丢失数据。 
重点： kafka 使用的是 ISR  follower 可以批量的从 leader 复制数据，而且 leader 充分利用磁盘的 顺序读
和 “零拷贝” 提升复制性能，减少 follower 和 leader 的消息差

** 什么情况下 broker 会从 isr 中踢出去

leader 会维护一个与其基本同步的 replica 列表， 这个列表就是 ISR（in sync replica）每个 pardition 都有
一个 ISR 有 leader 进行维护，如果一个 follwer 比一个 leader 落后太多，能活着超过一定的时间没有进行复制，那么
leader 就会将 其从 ISR 中移除。

** kafka 为什么快

1. cache FilesystemCache PageCache 缓存
2. 顺序写技术，操作系统提供的 预读，和写技术，磁盘的顺序写大多数情况比随机写内存还快
3. 零拷贝技术，减少拷贝次数
4. 批量处理，合并小的请求，以流的方式进行交互，
5. pull 拉的模式 使用拉的模式进行消息消费，消费 与处理端能力相同

** kafka producer 如何优化打入速度

增加线程
提高 batch.size
增加更多的 producer 实例
增加 partition 数量
设置 ack=-1 时 如果延时增大： 可以增大 num.replica.fetchers  follower 同步数据的线程数来调节
跨数据中心传输： 增加 socket 缓冲区设置，以及 os tcp 缓冲区设置

** kafka producer 打数据 ack 为 0 1 -1 时 代表的是啥， 设置-1 的时候什么情况下 leader 会认为一条消息 commit 了

1. 1 （默认） 数据发送到卡夫卡之后，经过 leader 成功接收消息的确认，就算发送成功了。这种情况 如果leader 宕机
则数据会丢失
2. 0 生产者将 数据发送之后就不管了，不等待任何返回。这种情况效率传输最高，但是数据可靠性最低
3. -1 producer 需要等待 ISR 中的所有follower 都确认接收到数据之后才算一次发送完成，可靠性最高。 当 ISR 中
所有的 replica 都想 leader 发送 ACK 时 leader 才 commit 这时producer 才认为一个消息 commit 了

** leader crash 时  ISR 为 空怎么办

unclean.leader.election 配置

true 默认允许同步副本成为 leader 由于不同步副本的消息较为滞后，成为 leader 可能会出现消息不一致的情况
false： 不允许不同步副本 成为 leader 此时如果发生 ISR 列表为空， 会一直等待 leader 恢复，降低可用性


** kafka 的 message 格式是 什么样子的

一个 kafka 的 message 由一个固定长度的 header 和一个 可变长度的 消息题 body 组成

header 部分由一个字节 的 magic 文件格式和 四个字节的 CRC32 用于判断 body 消息体是正常构成
当 magic 为 1 时  在  magic 和 CRC32 之间多一个字节的数据： attributes 保存一些相关属性
是否压缩，压缩格式等； 如果 magic 值为0 那么不存在 attributes 属性

body 是有N个字节构成的一个消息体，包含了具体的 key/value 消息

** kafka 中的 consumer group 是什么概念

同一个 topic 的数据，会广播给不同的 group 同一个 group 中的 worker 只有一个 worker 能拿到这个数据。
换句话说，对于同一个topic ，每个 group 都能拿到同样的数据，但是 进入到 group 之后只能被其中的 一个 
worker 消费， worker 的数量一般不超过 partition 的数量 一个partition 只能被一个 workder 消费

** 消息是否会丢失，和重复消费？

1. 消息发送

kafka 可以通过设置 ack 的类型来判断消息接收是否成功确认；
1. 0---表示不进行确认
2. 1---表示当 leader 接收成功时确认
3.-1---表示Leader 和 follower 都接收成功时确认；

当 acks=0 时 不和 kafka 集群进行消息接收确认，则当网络异常缓冲区满了等情况时可能丢失
当 acks=1 时 同步模式下 leader 确认成功但是挂掉，副本没有同步，数据可能丢失

2. 消息消费
kafka 提供两个 consumer 接口，一个 lowlevelapi 和 High-level-api

1. low-level API : 消费者自己维护 offset 可以实现对 kafka 的完全控制
2. high-level API : 封装了 partition 和 offset 的管理，使用简单

如果使用 high-level api 存在一个问题，就是 当消费者从集群当中把消息取出来，并且提交了新的 offset ，
还没来得及消费，服务就挂掉了，那么下次消费前没消费成功的消息就诡异的消失了；

解决方案：
消息丢失 可以将 acks 设置为 -1 将leader 和 follower 同步之后在在确认发送成功。
消费消息  将消息唯一标识保存到 外部介质，每次消费时判断是否处理过。

** kafka 为什么不支持读写分离

1. 数据一致性问题
2. 延时问题

** kafka 中是怎么体现消息顺序性的

kafka 每个partition中的消息在写入时都是有序的，消费时，每个partition 只能被每个 group 中的一个消费者
消费，整个 topic 不保证有序。如果为了保证topic 整个有序，那么将 partition 调整为 1

** 如何实现延迟队列？
** 事务如何实现
** 哪些地方用到了选举
